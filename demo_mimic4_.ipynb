{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuaLK: Tutorial for MIMIC-IV Dataset\n",
    "\n",
    "This notebook provides a comprehensive pipeline for reproducing the DuaLK model results on the MIMIC-IV dataset. The workflow encompasses four critical stages: data preprocessing, laboratory-based pretraining, auxiliary laboratory layer training, and final model fine-tuning with dual knowledge integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we import the necessary dependencies and configure the computational environment. The framework leverages PyTorch for deep learning operations and PyTorch Geometric for graph neural network components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Suppress sklearn warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_seed = 66\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(\"Environment initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "2.5.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The preprocessing stage implements a sophisticated pipeline that transforms raw MIMIC-IV clinical records into structured tensorial representations. This involves hierarchical patient-admission mapping, ICD code standardization, laboratory event aggregation, and graph-based disease relationship encoding.\n",
    "\n",
    "**Key Configuration Parameters:**\n",
    "- `dataset`: Target dataset identifier (mimic4)\n",
    "- `train_num`: Cardinality of the training cohort (8000)\n",
    "- `test_num`: Cardinality of the test cohort (1000)\n",
    "- `threshold`: Minimum code frequency threshold for inclusion (0.01)\n",
    "- `kg_embed_dims`: Dimensionality of knowledge graph embeddings ([2000])\n",
    "\n",
    "The preprocessing leverages the `Mimic4Parser` class from `parse_csv_4.py`, which handles ICD-10 to ICD-9 code mapping and temporal filtering based on anchor year constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.parse_csv_4 import Mimic4Parser, process_lab_events, process_lab_items\n",
    "from preprocess.encode import encode_code, encode_lab\n",
    "from preprocess.build_dataset import split_patients, build_code_xy, build_code_xy_pretrain, build_single_lab_xy\n",
    "from preprocess.build_dataset import build_heart_failure_y, get_rare_codes\n",
    "from preprocess.auxiliary import load_icd2name\n",
    "from preprocess.generate_graph import generate_disease_complication_edge_index, generate_disease_complication_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized preprocessing for mimic4 dataset.\n"
     ]
    }
   ],
   "source": [
    "# Configuration dictionary with dataset-specific hyperparameters\n",
    "conf = {\n",
    "    'mimic4': {\n",
    "        'parser': Mimic4Parser,\n",
    "        'train_num': 8000,\n",
    "        'test_num': 1000,\n",
    "        'threshold': 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "data_path = 'data'\n",
    "dataset = 'mimic4'  # Critical: Must be mimic4 for this tutorial\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "if not os.path.exists(raw_path):\n",
    "    os.makedirs(raw_path)\n",
    "    print('Please place the MIMIC-IV CSV files in `data/%s/raw`' % dataset)\n",
    "    raise FileNotFoundError(\"Raw data files not found.\")\n",
    "\n",
    "print(f\"Initialized preprocessing for {dataset} dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1: Patient-Admission Parsing\n",
    "\n",
    "The parser extracts patient-admission temporal sequences and diagnoses codes. The `Mimic4Parser` employs a bi-directional mapping strategy to convert ICD-10 codes to ICD-9 format, ensuring compatibility with established clinical ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ICD-10 to ICD-9 map ...\n",
      "loading patients anchor year ...\n",
      "parsing the csv file of admission ...\n",
      "\tselecting valid admission ...\n",
      "\t\t546028 in 546028 rows\n",
      "\t\tremaining 325593 rows\n",
      "\t325593 in 325593 rows\n",
      "parsing csv file of diagnosis ...\n",
      "\tmapping ICD-10 to ICD-9 ...\n",
      "\t\t6364520 in 6364520 rows\n",
      "\t6364520 in 6364520 rows\n",
      "calibrating patients by admission ...\n",
      "calibrating admission by patients ...\n",
      "Valid patients extracted: 140935\n",
      "Maximum admission sequence length: 90\n",
      "Maximum diagnostic codes per visit: 40\n"
     ]
    }
   ],
   "source": [
    "parser = conf[dataset]['parser'](raw_path)\n",
    "patient_admission, admission_codes = parser.parse()\n",
    "\n",
    "print('Valid patients extracted: %d' % len(patient_admission))\n",
    "max_admission_num = max([len(admissions) for admissions in patient_admission.values()])\n",
    "max_code_num_in_a_visit = max([len(codes) for codes in admission_codes.values()])\n",
    "print('Maximum admission sequence length: %d' % max_admission_num)\n",
    "print('Maximum diagnostic codes per visit: %d' % max_code_num_in_a_visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2: Laboratory Event Processing\n",
    "\n",
    "Laboratory measurements are aggregated per admission, filtering for abnormal flags. The `process_lab_events` function constructs a mapping from admission IDs to abnormal lab item IDs, while `process_lab_items` categorizes laboratory tests into semantic groups (hematology, chemistry, blood gas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum laboratory tests per admission: 150\n"
     ]
    }
   ],
   "source": [
    "admission_labs, lab_set = process_lab_events(raw_path, admission_codes, dataset)\n",
    "lab_category = process_lab_items(raw_path, lab_set, dataset)\n",
    "max_lab_num_in_a_visit = max([len(labs) for labs in admission_labs.values()])\n",
    "print('Maximum laboratory tests per admission: %d' % max_lab_num_in_a_visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 3: Code Encoding and Mapping\n",
    "\n",
    "The encoding phase transforms diagnostic codes and laboratory items into integer indices. Two code maps are generated: `code_map` for multi-visit patients and `code_map_pretrain` encompassing single-visit patients, facilitating differential pretraining strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding code ...\n",
      "Lab counts per category: {'hematology': 2736676, 'chemistry': 1699193, 'blood gas': 295003}\n",
      "Unique laboratory items: 445\n",
      "Diagnostic codes (multi-visit): 7337\n",
      "Diagnostic codes (pretrain): 7945\n"
     ]
    }
   ],
   "source": [
    "admission_codes_encoded, code_map, code_map_pretrain = encode_code(patient_admission, admission_codes)\n",
    "admission_labs_encoded, lab_map = encode_lab(admission_labs, lab_category)\n",
    "\n",
    "lab_num = len(lab_map['all'])\n",
    "code_num = len(code_map)\n",
    "code_num_pretrain = len(code_map_pretrain)\n",
    "\n",
    "print('Unique laboratory items: %d' % lab_num)\n",
    "print('Diagnostic codes (multi-visit): %d' % code_num)\n",
    "print('Diagnostic codes (pretrain): %d' % code_num_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 4: Knowledge Graph Initialization\n",
    "\n",
    "We leverage pre-trained HAKE (Hierarchy-Aware Knowledge Embedding) representations to initialize disease node embeddings. The ICD9CM ontology provides semantic anchoring for diagnostic codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 66 unmatched ICD9CM codes\n",
      "ICD9CM codes matched: 7945\n"
     ]
    }
   ],
   "source": [
    "resource_path = 'resources'\n",
    "icd2name, _ = load_icd2name(resource_path, code_map_pretrain)\n",
    "print('ICD9CM codes matched: %d' % len(icd2name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 5: Patient Cohort Stratification\n",
    "\n",
    "The `split_patients` function partitions patients into single-visit, pretrain, train, validation, and test cohorts based on visit frequency and random sampling with a fixed seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting pretrain, train, valid, and test pids ...\n",
      "There are 82377 single admission patients, 58558 multiple admission patients\n",
      "\t100%00%\n",
      "Single-visit samples: 82377\n",
      "Pretrain samples: 90377, Train: 8000, Valid: 49558, Test: 1000\n"
     ]
    }
   ],
   "source": [
    "single_pids, pretrain_pids, train_pids, valid_pids, test_pids = split_patients(\n",
    "    patient_admission=patient_admission,\n",
    "    admission_codes=admission_codes,\n",
    "    code_map=code_map,\n",
    "    train_num=conf[dataset]['train_num'],\n",
    "    test_num=conf[dataset]['test_num'],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('Single-visit samples: %d' % len(single_pids))\n",
    "print('Pretrain samples: %d, Train: %d, Valid: %d, Test: %d' %\n",
    "      (len(pretrain_pids), len(train_pids), len(valid_pids), len(test_pids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 6: Disease Complication Graph Construction\n",
    "\n",
    "The disease-disease interaction graph is constructed by analyzing sequential diagnosis patterns within patient trajectories. Edge weights are derived from co-occurrence statistics, capturing complication relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating disease-disease complication edge index...\n",
      "Vocabulary Size of codes: 7945\n",
      "Disease-disease edges: 1725287\n"
     ]
    }
   ],
   "source": [
    "tuples_disease2disease = generate_disease_complication_edge_index(\n",
    "    pretrain_pids, patient_admission,\n",
    "    admission_codes_encoded, code_num_pretrain,\n",
    "    self_edge=True, edge_score=1\n",
    ")\n",
    "edge_index_disease2disease, edge_weight_disease2disease = tuples_disease2disease\n",
    "print('Disease-disease edges: %d' % len(edge_index_disease2disease[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 7: HAKE Embedding Initialization\n",
    "\n",
    "HAKE embeddings encode hierarchical disease relationships within a hyperbolic space, capturing both semantic similarity and taxonomic structure. The 2000-dimensional embedding space provides rich representational capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HAKE 2000-dimensional embeddings ...\n",
      "Cannot find: 213\n"
     ]
    }
   ],
   "source": [
    "emb_path = os.path.join(data_path, 'emb')\n",
    "kg_embed_dims = [2000]\n",
    "x_disease2disease_HAKE = {}\n",
    "\n",
    "for kg_embed_dim in kg_embed_dims:\n",
    "    print('Initializing HAKE %d-dimensional embeddings ...' % kg_embed_dim)\n",
    "    icd2hake = pickle.load(open(os.path.join(emb_path, f'ICD2HAKE_{kg_embed_dim}.pkl'), 'rb'))\n",
    "    x_disease2disease_HAKE[kg_embed_dim] = generate_disease_complication_x(\n",
    "        icd2hake, code_map_pretrain, emb_dim=kg_embed_dim\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 8: Dataset Construction\n",
    "\n",
    "This phase generates tensorial datasets for different training stages:\n",
    "- **Single-visit lab data**: For auxiliary laboratory prediction task\n",
    "- **Pretrain data**: Multi-category lab prediction (all, hematology, chemistry, blood gas)\n",
    "- **Train/Valid/Test data**: Diagnosis prediction with lab features\n",
    "- **Rare disease subset**: Focuses on low-frequency diagnostic codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building single lab features and labels ...\n",
      "Single-visit lab data shape: (82377, 445) (82377, 7945)\n"
     ]
    }
   ],
   "source": [
    "# Single-visit laboratory data\n",
    "lab_x, lab_y = build_single_lab_xy(\n",
    "    single_pids, patient_admission,\n",
    "    admission_codes_encoded, admission_labs_encoded['all']\n",
    ")\n",
    "lab_x, lab_y = shuffle(lab_x, lab_y, random_state=66)\n",
    "single_train_labs_x = lab_x[:int(0.85 * lab_x.shape[0])]\n",
    "single_train_labs_y = lab_y[:int(0.85 * lab_y.shape[0])]\n",
    "single_test_labs_x = lab_x[int(0.85 * lab_x.shape[0]):]\n",
    "single_test_labs_y = lab_y[int(0.85 * lab_y.shape[0]):]\n",
    "\n",
    "print('Single-visit lab data shape:', lab_x.shape, lab_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building pretrain codes features and labels ...\n",
      "\t90377 / 90377\n",
      "building pretrain codes features and labels ...\n",
      "\t90377 / 90377\n",
      "building pretrain codes features and labels ...\n",
      "\t90377 / 90377\n",
      "building pretrain codes features and labels ...\n",
      "\t90377 / 90377\n",
      "Pretrain data shape: (90377, 90, 40)\n"
     ]
    }
   ],
   "source": [
    "# Pretrain dataset with multi-category lab predictions\n",
    "pretrain_codes = build_code_xy_pretrain(\n",
    "    pretrain_pids, patient_admission, admission_codes_encoded,\n",
    "    admission_labs_encoded, max_admission_num, lab_map,\n",
    "    max_code_num_in_a_visit, lab_category='all'\n",
    ")\n",
    "pretrain_codes_x, pretrain_codes_y_all, pretrain_visit_lens = pretrain_codes\n",
    "\n",
    "_, pretrain_codes_y_hema, _ = build_code_xy_pretrain(\n",
    "    pretrain_pids, patient_admission, admission_codes_encoded,\n",
    "    admission_labs_encoded, max_admission_num, lab_map,\n",
    "    max_code_num_in_a_visit, lab_category='hematology'\n",
    ")\n",
    "_, pretrain_codes_y_chem, _ = build_code_xy_pretrain(\n",
    "    pretrain_pids, patient_admission, admission_codes_encoded,\n",
    "    admission_labs_encoded, max_admission_num, lab_map,\n",
    "    max_code_num_in_a_visit, lab_category='chemistry'\n",
    ")\n",
    "_, pretrain_codes_y_blood, _ = build_code_xy_pretrain(\n",
    "    pretrain_pids, patient_admission, admission_codes_encoded,\n",
    "    admission_labs_encoded, max_admission_num, lab_map,\n",
    "    max_code_num_in_a_visit, lab_category='blood gas'\n",
    ")\n",
    "\n",
    "pretrain_codes_y = pretrain_codes_y_all, pretrain_codes_y_hema, pretrain_codes_y_chem, pretrain_codes_y_blood\n",
    "print('Pretrain data shape:', pretrain_codes_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train/valid/test codes features and labels ...\n",
      "\t8000 / 8000\n",
      "building train/valid/test codes features and labels ...\n",
      "\t49558 / 49558\n",
      "building train/valid/test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "Train/Valid/Test shapes: (8000, 7337) (49558, 7337) (1000, 7337)\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid/Test datasets\n",
    "train_codes_x, train_codes_y, train_visit_lens, train_labs_x = build_code_xy(\n",
    "    train_pids, patient_admission,\n",
    "    admission_labs_encoded['all'], lab_num,\n",
    "    admission_codes_encoded,\n",
    "    max_admission_num,\n",
    "    code_num, max_code_num_in_a_visit\n",
    ")\n",
    "valid_codes_x, valid_codes_y, valid_visit_lens, valid_labs_x = build_code_xy(\n",
    "    valid_pids, patient_admission,\n",
    "    admission_labs_encoded['all'], lab_num,\n",
    "    admission_codes_encoded,\n",
    "    max_admission_num,\n",
    "    code_num, max_code_num_in_a_visit\n",
    ")\n",
    "test_codes_x, test_codes_y, test_visit_lens, test_labs_x = build_code_xy(\n",
    "    test_pids, patient_admission,\n",
    "    admission_labs_encoded['all'], lab_num,\n",
    "    admission_codes_encoded,\n",
    "    max_admission_num,\n",
    "    code_num, max_code_num_in_a_visit\n",
    ")\n",
    "\n",
    "print('Train/Valid/Test shapes:', train_codes_y.shape, valid_codes_y.shape, test_codes_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting rare disease codes (threshold=3)...\n",
      "Rare disease shapes: (8000, 4424) (49558, 4424) (1000, 4424)\n"
     ]
    }
   ],
   "source": [
    "# Rare disease subset extraction\n",
    "print('Extracting rare disease codes (threshold=3)...')\n",
    "codes_y_rare = get_rare_codes(train_codes_y, valid_codes_y, test_codes_y, threshold=3)\n",
    "train_codes_y_r, valid_codes_y_r, test_codes_y_r = codes_y_rare\n",
    "print('Rare disease shapes:', train_codes_y_r.shape, valid_codes_y_r.shape, test_codes_y_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train/valid/test heart failure labels ...\n",
      "building train/valid/test heart failure labels ...\n",
      "building train/valid/test heart failure labels ...\n"
     ]
    }
   ],
   "source": [
    "# Heart failure outcome labels (ICD-9 code 428)\n",
    "train_hf_y = build_heart_failure_y('428', train_codes_y, code_map)\n",
    "valid_hf_y = build_heart_failure_y('428', valid_codes_y, code_map)\n",
    "test_hf_y = build_heart_failure_y('428', test_codes_y, code_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 9: Data Persistence\n",
    "\n",
    "Serialized datasets are stored in structured directories for subsequent training phases. This modular approach decouples preprocessing from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting encoded data...\n",
      "Persisting standard datasets...\n",
      "Preprocessing completed. All datasets saved.\n"
     ]
    }
   ],
   "source": [
    "train_labs_data = (single_train_labs_x, single_train_labs_y)\n",
    "test_labs_data = (single_test_labs_x, single_test_labs_y)\n",
    "pretrain_codes_data = (pretrain_codes_x, pretrain_codes_y, None, pretrain_visit_lens)\n",
    "train_codes_data = (train_codes_x, train_codes_y, train_labs_x, train_codes_y_r)\n",
    "valid_codes_data = (valid_codes_x, valid_codes_y, valid_labs_x, valid_codes_y_r)\n",
    "test_codes_data = (test_codes_x, test_codes_y, test_labs_x, test_codes_y_r)\n",
    "\n",
    "encoded_path = os.path.join(dataset_path, 'encoded')\n",
    "os.makedirs(encoded_path, exist_ok=True)\n",
    "\n",
    "print('Persisting encoded data...')\n",
    "pickle.dump(patient_admission, open(os.path.join(encoded_path, 'patient_admission.pkl'), 'wb'))\n",
    "pickle.dump(admission_codes_encoded, open(os.path.join(encoded_path, 'codes_encoded.pkl'), 'wb'))\n",
    "pickle.dump(admission_labs_encoded, open(os.path.join(encoded_path, 'labs_encoded.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'lab_map': lab_map,\n",
    "    'code_map': code_map,\n",
    "    'code_map_pretrain': code_map_pretrain\n",
    "}, open(os.path.join(encoded_path, 'code_maps.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'pretrain_pids': pretrain_pids,\n",
    "    'train_pids': train_pids,\n",
    "    'valid_pids': valid_pids,\n",
    "    'test_pids': test_pids\n",
    "}, open(os.path.join(encoded_path, 'pids.pkl'), 'wb'))\n",
    "\n",
    "standard_path = os.path.join(dataset_path, 'standard')\n",
    "os.makedirs(standard_path, exist_ok=True)\n",
    "\n",
    "print('Persisting standard datasets...')\n",
    "pickle.dump(pretrain_codes_data, open(os.path.join(standard_path, 'pretrain_codes_dataset.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'train_labs_data': train_labs_data,\n",
    "    'test_labs_data': test_labs_data\n",
    "}, open(os.path.join(standard_path, 'labs_dataset.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'train_codes_data': train_codes_data,\n",
    "    'valid_codes_data': valid_codes_data,\n",
    "    'test_codes_data': test_codes_data\n",
    "}, open(os.path.join(standard_path, 'codes_dataset.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'train_hf_y': train_hf_y,\n",
    "    'valid_hf_y': valid_hf_y,\n",
    "    'test_hf_y': test_hf_y\n",
    "}, open(os.path.join(standard_path, 'heart_failure.pkl'), 'wb'))\n",
    "\n",
    "graph_path = os.path.join(dataset_path, 'graph')\n",
    "os.makedirs(graph_path, exist_ok=True)\n",
    "\n",
    "pickle.dump({\n",
    "    'edge_index': edge_index_disease2disease,\n",
    "    'edge_weight': edge_weight_disease2disease,\n",
    "    'x_hake_2000': x_disease2disease_HAKE[2000],\n",
    "}, open(os.path.join(graph_path, 'disease2disease.pkl'), 'wb'))\n",
    "\n",
    "print('Preprocessing completed. All datasets saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratory Pretraining\n",
    "\n",
    "The laboratory pretraining phase employs a multi-decoder architecture to learn category-specific laboratory prediction tasks. This stage consists of two sub-phases:\n",
    "\n",
    "1. **Joint Pretraining**: All decoders (hematology, chemistry, blood gas) are trained simultaneously with a unified loss function, establishing shared representational foundations.\n",
    "2. **Individual Decoder Refinement**: Each decoder is fine-tuned independently to capture category-specific patterns.\n",
    "\n",
    "**Architectural Specifications:**\n",
    "- `init_dim`: 2000 (HAKE embedding dimensionality)\n",
    "- `GNN.type`: 'gat' (Graph Attention Network for disease graph encoding)\n",
    "- `GNN.dims`: (256, 256) (two-layer GAT with 256 hidden units)\n",
    "- `Decoder.dims`: (256, 128) (hierarchical decoder architecture)\n",
    "- `joint_epochs`: 10 (iterations for joint training)\n",
    "- `individual_epochs`: 10 (iterations per decoder refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models.model import DuaLK\n",
    "from utils import PatientLabDataset, load_data\n",
    "from utils import pretrain_model_jointly, pretrain_individual_decoder\n",
    "\n",
    "model_config = {\n",
    "    'init_dim': 2000,\n",
    "    'GNN': {\n",
    "        'type': 'gat',\n",
    "        'dims': (256, 256),\n",
    "        'dropout': 0.,\n",
    "    },\n",
    "    'Attention': {\n",
    "        'dropout': 0.2,\n",
    "    },\n",
    "    'Decoder': {\n",
    "        'dims': (256, 128),\n",
    "        'dropout': 0.4,\n",
    "    },\n",
    "    'Classifier': {\n",
    "        'dims': [256],\n",
    "        'dropout': 0.4,\n",
    "    }\n",
    "}\n",
    "\n",
    "data_path, dataset = 'data', 'mimic4'\n",
    "pretrain = True\n",
    "train_type = 'pretrain'\n",
    "use_lab = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "code_fuse, visit_fuse = 'simple', 'simple'\n",
    "gnn_type = model_config['GNN']['type']\n",
    "\n",
    "print(f'Pretraining on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Preprocessed Data\n",
    "\n",
    "The `load_data` utility function retrieves graph structures, node features, and pretrain labels. Note the separation of laboratory categories for specialized decoder training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph node features shape: torch.Size([7946, 2000])\n",
      "Training data shape: (90377, 90, 40)\n"
     ]
    }
   ],
   "source": [
    "train_codes_x, pretrain_codes_y, edge_index, x, edge_weight = load_data(\n",
    "    pretrain, data_path, dataset, model_config['init_dim']\n",
    ")\n",
    "_, pretrain_codes_y_hema, pretrain_codes_y_chem, pretrain_codes_y_blood = pretrain_codes_y\n",
    "\n",
    "x = x.float()\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print('Graph node features shape:', data.x.shape)\n",
    "print('Training data shape:', train_codes_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset construction with category-specific labels\n",
    "train_dataset = PatientLabDataset(\n",
    "    train_codes_x, pretrain_codes_y_hema,\n",
    "    pretrain_codes_y_chem, pretrain_codes_y_blood\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Validation subset for monitoring\n",
    "test_indices = np.random.choice(len(train_dataset), 10000, replace=False)\n",
    "test_subset = torch.utils.data.Subset(train_dataset, test_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Instantiation\n",
    "\n",
    "The DuaLK model integrates graph neural encoding with attention-based visit aggregation. The `num_classes` parameter specifies output dimensions for each laboratory category decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuaLK(\n",
      "  (gnn_layer): GATLayer(\n",
      "    (conv1): GATConv(2000, 256, heads=1)\n",
      "    (conv2): GATConv(256, 256, heads=1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (visit_attention): AttentionLayer(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (patient_attention): AttentionLayer(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (decoder1): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=239, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (decoder2): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=191, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (decoder3): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=15, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "joint_epochs = 1 # Here we set 1 epoch for quick start\n",
    "individual_epochs = 1 # Here we set 1 epoch for quick start\n",
    "num_classes = [\n",
    "    pretrain_codes_y_hema.shape[1],\n",
    "    pretrain_codes_y_chem.shape[1],\n",
    "    pretrain_codes_y_blood.shape[1]\n",
    "]\n",
    "\n",
    "data = data.to(str(device))\n",
    "model = DuaLK(\n",
    "    model_config=model_config,\n",
    "    emb_init=data.x,\n",
    "    num_classes=num_classes,\n",
    "    use_lab=use_lab,\n",
    "    code_fuse=code_fuse,\n",
    "    visit_fuse=visit_fuse,\n",
    "    train_type=train_type,\n",
    "    lab_weight=None,\n",
    "    lab_bias=None,\n",
    "    gnn_type=gnn_type\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Pretraining Phase\n",
    "\n",
    "Multi-task learning across laboratory categories enforces shared feature extraction while preserving category-specific nuances through dedicated output heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating joint pretraining...\n",
      "Epoch 1/1, Train Loss: 0.4419, Test Loss: 0.3148, F1-weighted1: 0.6341, F1-weighted2: 0.5906, F1-weighted3: 0.7932, Learning Rate: 0.00100, Train Time: 807.07s, Eval Time: 90.37s\n",
      "Joint pretrained model saved: joint_pretrained_model_1.pth\n"
     ]
    }
   ],
   "source": [
    "step = 'together'  # Options: 'joint', 'individual', 'together'\n",
    "\n",
    "if step == \"joint\" or step == 'together':\n",
    "    print('Initiating joint pretraining...')\n",
    "    pretrain_model_jointly(\n",
    "        model, data, train_loader, criterion,\n",
    "        optimizer, joint_epochs, device, test_loader\n",
    "    )\n",
    "    torch.save(model.state_dict(), f'./saved/joint_pretrained_model_{joint_epochs}.pth')\n",
    "    print(f'Joint pretrained model saved: joint_pretrained_model_{joint_epochs}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Decoder Refinement\n",
    "\n",
    "Post joint training, each decoder undergoes isolated optimization to maximize category-specific predictive performance. This two-stage approach balances generalization and specialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating individual decoder refinement...\n",
      "\n",
      "Refining hema decoder...\n",
      "Epoch 1/1, Train Loss (hema): 0.0780, Test Loss: 0.0752, F1-weighted: 0.6369, Learning Rate: 0.00100, Train Time: 777.26s, Eval Time: 74.29s\n",
      "\n",
      "Refining chem decoder...\n",
      "Epoch 1/1, Train Loss (chem): 0.0668, Test Loss: 0.0637, F1-weighted: 0.5928, Learning Rate: 0.00100, Train Time: 704.37s, Eval Time: 87.76s\n",
      "\n",
      "Refining blood decoder...\n",
      "Epoch 1/1, Train Loss (blood): 0.1817, Test Loss: 0.1700, F1-weighted: 0.7946, Learning Rate: 0.00100, Train Time: 797.91s, Eval Time: 90.06s\n",
      "\n",
      "Individual decoder weights saved.\n"
     ]
    }
   ],
   "source": [
    "if step == \"individual\" or step == 'together':\n",
    "    model.load_state_dict(torch.load(f'./saved/joint_pretrained_model_{joint_epochs}.pth'))\n",
    "    print('Initiating individual decoder refinement...')\n",
    "    decoders = ['hema', 'chem', 'blood']\n",
    "    for decoder_type in decoders:\n",
    "        print(f'\\nRefining {decoder_type} decoder...')\n",
    "        pretrain_individual_decoder(\n",
    "            model, data, train_loader, criterion,\n",
    "            optimizer, individual_epochs, device,\n",
    "            decoder_type, test_loader\n",
    "        )\n",
    "    print('\\nIndividual decoder weights saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Laboratory Layer Training\n",
    "\n",
    "This stage trains a standalone feed-forward network to map laboratory values to diagnostic codes using single-visit data. The learned weights initialize the laboratory integration module in the final model, providing a warm start for lab-diagnosis associations.\n",
    "\n",
    "**Network Architecture:**\n",
    "- Input dimension: Variable (number of lab items, ~239 for MIMIC-IV)\n",
    "- Hidden layer: 256 units with ReLU activation and 0.4 dropout\n",
    "- Output dimension: Number of diagnostic codes\n",
    "\n",
    "**Training Configuration:**\n",
    "- `batch_size`: 128\n",
    "- `learning_rate`: 0.001 with StepLR decay (Î³=0.5 every 20 epochs)\n",
    "- `num_epochs`: 100\n",
    "- Evaluation metrics: Recall@20, Recall@40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import top_k_prec_recall\n",
    "\n",
    "def train_and_evaluate_lab(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    \"\"\"Training loop for auxiliary laboratory prediction layer.\"\"\"\n",
    "    ks = [20, 40]\n",
    "    best_test_loss = float('inf')\n",
    "    best_model_path = './saved/train_lab/lab_layer_checkpoint.pth'\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        y_true_test = []\n",
    "        y_pred_test = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                y_true_test.append(labels.cpu().numpy())\n",
    "                y_pred_test.append(outputs.cpu().numpy())\n",
    "\n",
    "        y_true_test = np.vstack(y_true_test)\n",
    "        y_pred_test = np.vstack(y_pred_test)\n",
    "        y_pred_sorted_test = np.argsort(y_pred_test, axis=1)[:, ::-1]\n",
    "\n",
    "        _, recall_at_k_test = top_k_prec_recall(y_true_test, y_pred_sorted_test, ks)\n",
    "        test_loss /= len(test_loader)\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Test Loss: {test_loss:.4f}, '\n",
    "              f'Test Recall@20: {recall_at_k_test[0]:.4f}, '\n",
    "              f'Test Recall@40: {recall_at_k_test[1]:.4f}, '\n",
    "              f'LR: {current_lr:.6f}')\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save({\n",
    "                'linear1_weight': model[0].weight.data,\n",
    "                'linear1_bias': model[0].bias.data,\n",
    "                'linear2_weight': model[3].weight.data,\n",
    "                'linear2_bias': model[3].bias.data,\n",
    "            }, best_model_path)\n",
    "            print(f'Checkpoint saved at epoch {epoch + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single-visit laboratory data...\n",
      "Train lab shape: torch.Size([70020, 445]) torch.Size([70020, 7945])\n",
      "Test lab shape: torch.Size([12357, 445]) torch.Size([12357, 7945])\n",
      "\n",
      "Training auxiliary laboratory layer...\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.0510, Test Loss: 0.0066, Test Recall@20: 0.3652, Test Recall@40: 0.4549, LR: 0.001000\n",
      "Checkpoint saved at epoch 1\n",
      "Epoch 2/100, Train Loss: 0.0052, Test Loss: 0.0046, Test Recall@20: 0.3840, Test Recall@40: 0.4876, LR: 0.001000\n",
      "Checkpoint saved at epoch 2\n",
      "Epoch 3/100, Train Loss: 0.0044, Test Loss: 0.0042, Test Recall@20: 0.3973, Test Recall@40: 0.4997, LR: 0.001000\n",
      "Checkpoint saved at epoch 3\n",
      "Epoch 4/100, Train Loss: 0.0042, Test Loss: 0.0041, Test Recall@20: 0.4101, Test Recall@40: 0.5078, LR: 0.001000\n",
      "Checkpoint saved at epoch 4\n",
      "Epoch 5/100, Train Loss: 0.0041, Test Loss: 0.0040, Test Recall@20: 0.4206, Test Recall@40: 0.5201, LR: 0.001000\n",
      "Checkpoint saved at epoch 5\n",
      "Epoch 6/100, Train Loss: 0.0040, Test Loss: 0.0040, Test Recall@20: 0.4251, Test Recall@40: 0.5277, LR: 0.001000\n",
      "Checkpoint saved at epoch 6\n",
      "Epoch 7/100, Train Loss: 0.0040, Test Loss: 0.0039, Test Recall@20: 0.4311, Test Recall@40: 0.5339, LR: 0.001000\n",
      "Checkpoint saved at epoch 7\n",
      "Epoch 8/100, Train Loss: 0.0039, Test Loss: 0.0039, Test Recall@20: 0.4347, Test Recall@40: 0.5392, LR: 0.001000\n",
      "Checkpoint saved at epoch 8\n",
      "Epoch 9/100, Train Loss: 0.0039, Test Loss: 0.0039, Test Recall@20: 0.4355, Test Recall@40: 0.5426, LR: 0.001000\n",
      "Checkpoint saved at epoch 9\n",
      "Epoch 10/100, Train Loss: 0.0039, Test Loss: 0.0039, Test Recall@20: 0.4416, Test Recall@40: 0.5426, LR: 0.001000\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch 11/100, Train Loss: 0.0038, Test Loss: 0.0039, Test Recall@20: 0.4403, Test Recall@40: 0.5477, LR: 0.001000\n",
      "Checkpoint saved at epoch 11\n",
      "Epoch 12/100, Train Loss: 0.0038, Test Loss: 0.0039, Test Recall@20: 0.4426, Test Recall@40: 0.5503, LR: 0.001000\n",
      "Checkpoint saved at epoch 12\n",
      "Epoch 13/100, Train Loss: 0.0038, Test Loss: 0.0039, Test Recall@20: 0.4438, Test Recall@40: 0.5504, LR: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.0038, Test Loss: 0.0039, Test Recall@20: 0.4460, Test Recall@40: 0.5528, LR: 0.001000\n",
      "Checkpoint saved at epoch 14\n",
      "Epoch 15/100, Train Loss: 0.0038, Test Loss: 0.0038, Test Recall@20: 0.4459, Test Recall@40: 0.5538, LR: 0.001000\n",
      "Checkpoint saved at epoch 15\n",
      "Epoch 16/100, Train Loss: 0.0038, Test Loss: 0.0038, Test Recall@20: 0.4464, Test Recall@40: 0.5528, LR: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.0038, Test Loss: 0.0038, Test Recall@20: 0.4483, Test Recall@40: 0.5567, LR: 0.001000\n",
      "Checkpoint saved at epoch 17\n",
      "Epoch 18/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4497, Test Recall@40: 0.5558, LR: 0.001000\n",
      "Checkpoint saved at epoch 18\n",
      "Epoch 19/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4489, Test Recall@40: 0.5555, LR: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4509, Test Recall@40: 0.5566, LR: 0.000500\n",
      "Epoch 21/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4512, Test Recall@40: 0.5583, LR: 0.000500\n",
      "Checkpoint saved at epoch 21\n",
      "Epoch 22/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4495, Test Recall@40: 0.5580, LR: 0.000500\n",
      "Checkpoint saved at epoch 22\n",
      "Epoch 23/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4520, Test Recall@40: 0.5593, LR: 0.000500\n",
      "Checkpoint saved at epoch 23\n",
      "Epoch 24/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4521, Test Recall@40: 0.5595, LR: 0.000500\n",
      "Epoch 25/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4525, Test Recall@40: 0.5584, LR: 0.000500\n",
      "Epoch 26/100, Train Loss: 0.0037, Test Loss: 0.0038, Test Recall@20: 0.4516, Test Recall@40: 0.5578, LR: 0.000500\n",
      "Epoch 27/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4518, Test Recall@40: 0.5569, LR: 0.000500\n",
      "Epoch 28/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4534, Test Recall@40: 0.5600, LR: 0.000500\n",
      "Epoch 29/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4527, Test Recall@40: 0.5607, LR: 0.000500\n",
      "Epoch 30/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4520, Test Recall@40: 0.5602, LR: 0.000500\n",
      "Epoch 31/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4524, Test Recall@40: 0.5597, LR: 0.000500\n",
      "Epoch 32/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4528, Test Recall@40: 0.5600, LR: 0.000500\n",
      "Epoch 33/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4531, Test Recall@40: 0.5591, LR: 0.000500\n",
      "Epoch 34/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4527, Test Recall@40: 0.5607, LR: 0.000500\n",
      "Epoch 35/100, Train Loss: 0.0036, Test Loss: 0.0038, Test Recall@20: 0.4520, Test Recall@40: 0.5585, LR: 0.000500\n",
      "Epoch 36/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4531, Test Recall@40: 0.5608, LR: 0.000500\n",
      "Epoch 37/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4531, Test Recall@40: 0.5599, LR: 0.000500\n",
      "Epoch 38/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4533, Test Recall@40: 0.5598, LR: 0.000500\n",
      "Epoch 39/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4531, Test Recall@40: 0.5585, LR: 0.000500\n",
      "Epoch 40/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4531, Test Recall@40: 0.5599, LR: 0.000250\n",
      "Epoch 41/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5597, LR: 0.000250\n",
      "Epoch 42/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5599, LR: 0.000250\n",
      "Epoch 43/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4528, Test Recall@40: 0.5595, LR: 0.000250\n",
      "Epoch 44/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4540, Test Recall@40: 0.5605, LR: 0.000250\n",
      "Epoch 45/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4530, Test Recall@40: 0.5599, LR: 0.000250\n",
      "Epoch 46/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5597, LR: 0.000250\n",
      "Epoch 47/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5596, LR: 0.000250\n",
      "Epoch 48/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4532, Test Recall@40: 0.5600, LR: 0.000250\n",
      "Epoch 49/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5606, LR: 0.000250\n",
      "Epoch 50/100, Train Loss: 0.0036, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5597, LR: 0.000250\n",
      "Epoch 51/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4530, Test Recall@40: 0.5601, LR: 0.000250\n",
      "Epoch 52/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4530, Test Recall@40: 0.5603, LR: 0.000250\n",
      "Epoch 53/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4530, Test Recall@40: 0.5606, LR: 0.000250\n",
      "Epoch 54/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4540, Test Recall@40: 0.5601, LR: 0.000250\n",
      "Epoch 55/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5601, LR: 0.000250\n",
      "Epoch 56/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5606, LR: 0.000250\n",
      "Epoch 57/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4532, Test Recall@40: 0.5593, LR: 0.000250\n",
      "Epoch 58/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4540, Test Recall@40: 0.5600, LR: 0.000250\n",
      "Epoch 59/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5597, LR: 0.000250\n",
      "Epoch 60/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4539, Test Recall@40: 0.5600, LR: 0.000125\n",
      "Epoch 61/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4544, Test Recall@40: 0.5604, LR: 0.000125\n",
      "Epoch 62/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4549, Test Recall@40: 0.5603, LR: 0.000125\n",
      "Epoch 63/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4536, Test Recall@40: 0.5604, LR: 0.000125\n",
      "Epoch 64/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4540, Test Recall@40: 0.5600, LR: 0.000125\n",
      "Epoch 65/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4541, Test Recall@40: 0.5603, LR: 0.000125\n",
      "Epoch 66/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4540, Test Recall@40: 0.5603, LR: 0.000125\n",
      "Epoch 67/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4532, Test Recall@40: 0.5599, LR: 0.000125\n",
      "Epoch 68/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4533, Test Recall@40: 0.5604, LR: 0.000125\n",
      "Epoch 69/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5613, LR: 0.000125\n",
      "Epoch 70/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5605, LR: 0.000125\n",
      "Epoch 71/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4533, Test Recall@40: 0.5602, LR: 0.000125\n",
      "Epoch 72/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5602, LR: 0.000125\n",
      "Epoch 73/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5606, LR: 0.000125\n",
      "Epoch 74/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4532, Test Recall@40: 0.5602, LR: 0.000125\n",
      "Epoch 75/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4543, Test Recall@40: 0.5608, LR: 0.000125\n",
      "Epoch 76/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4539, Test Recall@40: 0.5605, LR: 0.000125\n",
      "Epoch 77/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5606, LR: 0.000125\n",
      "Epoch 78/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4532, Test Recall@40: 0.5603, LR: 0.000125\n",
      "Epoch 79/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5608, LR: 0.000125\n",
      "Epoch 80/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4536, Test Recall@40: 0.5612, LR: 0.000063\n",
      "Epoch 81/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5609, LR: 0.000063\n",
      "Epoch 82/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4536, Test Recall@40: 0.5605, LR: 0.000063\n",
      "Epoch 83/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4533, Test Recall@40: 0.5607, LR: 0.000063\n",
      "Epoch 84/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4533, Test Recall@40: 0.5608, LR: 0.000063\n",
      "Epoch 85/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5608, LR: 0.000063\n",
      "Epoch 86/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5605, LR: 0.000063\n",
      "Epoch 87/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5607, LR: 0.000063\n",
      "Epoch 88/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5608, LR: 0.000063\n",
      "Epoch 89/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5610, LR: 0.000063\n",
      "Epoch 90/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5608, LR: 0.000063\n",
      "Epoch 91/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5612, LR: 0.000063\n",
      "Epoch 92/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5604, LR: 0.000063\n",
      "Epoch 93/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4537, Test Recall@40: 0.5611, LR: 0.000063\n",
      "Epoch 94/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5607, LR: 0.000063\n",
      "Epoch 95/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4538, Test Recall@40: 0.5604, LR: 0.000063\n",
      "Epoch 96/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4536, Test Recall@40: 0.5610, LR: 0.000063\n",
      "Epoch 97/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5605, LR: 0.000063\n",
      "Epoch 98/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5612, LR: 0.000063\n",
      "Epoch 99/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4535, Test Recall@40: 0.5605, LR: 0.000063\n",
      "Epoch 100/100, Train Loss: 0.0035, Test Loss: 0.0039, Test Recall@20: 0.4534, Test Recall@40: 0.5606, LR: 0.000031\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "dataset = 'mimic4'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Loading single-visit laboratory data...')\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "standard_path = os.path.join(dataset_path, 'standard')\n",
    "labs_dataset = pickle.load(open(os.path.join(standard_path, 'labs_dataset.pkl'), 'rb'))\n",
    "train_labs_x, train_labs_y = labs_dataset['train_labs_data']\n",
    "test_labs_x, test_labs_y = labs_dataset['test_labs_data']\n",
    "\n",
    "train_labs_x, train_labs_y = torch.from_numpy(train_labs_x), torch.from_numpy(train_labs_y)\n",
    "test_labs_x, test_labs_y = torch.from_numpy(test_labs_x), torch.from_numpy(test_labs_y)\n",
    "print('Train lab shape:', train_labs_x.shape, train_labs_y.shape)\n",
    "print('Test lab shape:', test_labs_x.shape, test_labs_y.shape)\n",
    "\n",
    "item_num = train_labs_x.shape[1]\n",
    "code_num = train_labs_y.shape[1]\n",
    "\n",
    "lab_model = nn.Sequential(\n",
    "    nn.Linear(item_num, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, code_num),\n",
    ").to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_labs_x.float(), train_labs_y.float())\n",
    "test_dataset = TensorDataset(test_labs_x.float(), test_labs_y.float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.Adam(lab_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "num_epochs = 100\n",
    "print('\\nTraining auxiliary laboratory layer...\\n')\n",
    "train_and_evaluate_lab(lab_model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final laboratory layer checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./saved/train_lab', exist_ok=True)\n",
    "end_model_path = './saved/train_lab/lab_layer_checkpoint_end.pth'\n",
    "torch.save({\n",
    "    'linear1_weight': lab_model[0].weight.data,\n",
    "    'linear1_bias': lab_model[0].bias.data,\n",
    "    'linear2_weight': lab_model[3].weight.data,\n",
    "    'linear2_bias': lab_model[3].bias.data,\n",
    "}, end_model_path)\n",
    "print('Final laboratory layer checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Training (Fine-tuning)\n",
    "\n",
    "The culminating training phase integrates all pretrained components into the full DuaLK architecture. This involves:\n",
    "\n",
    "1. **Initialization**: Loading pretrained graph encoder and decoder weights\n",
    "2. **Laboratory Integration**: Incorporating learned lab-diagnosis mappings\n",
    "3. **End-to-End Optimization**: Fine-tuning on multi-visit diagnosis prediction\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `train_type`: 'finetune' (enables partial parameter freezing)\n",
    "- `use_lab`: True (activates laboratory feature fusion)\n",
    "- `loss`: 'pos_weight' (balanced binary cross-entropy)\n",
    "- `code_fuse/visit_fuse`: 'simple' (concatenation-based fusion)\n",
    "- `epochs`: 500 with adaptive learning rate decay\n",
    "- `batch_size`: 32\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- F1-weighted: Macro-averaged F1 score across all diagnostic codes\n",
    "- Recall@k (k=10,20,30,40): Top-k retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PatientDataset, load_data\n",
    "from metrics import f1\n",
    "from models.loss import WeightedBCEWithLogitsLoss\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr):\n",
    "    \"\"\"Stepwise learning rate decay schedule.\"\"\"\n",
    "    if epoch > 40:\n",
    "        lr = 0.0001\n",
    "    elif epoch > 30:\n",
    "        lr = 0.0005\n",
    "    else:\n",
    "        lr = base_lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss configuration: pos_weight=1, neg_weight=1\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'init_dim': 2000,\n",
    "    'GNN': {\n",
    "        'type': 'gat',\n",
    "        'dims': (256, 256),\n",
    "        'dropout': 0.,\n",
    "    },\n",
    "    'Attention': {\n",
    "        'dropout': 0.2,\n",
    "    },\n",
    "    'Decoder': {\n",
    "        'dims': (256, 128),\n",
    "        'dropout': 0.4,\n",
    "    },\n",
    "    'Classifier': {\n",
    "        'dims': [256],\n",
    "        'dropout': 0.4,\n",
    "    }\n",
    "}\n",
    "\n",
    "data_path, dataset = 'data', 'mimic4'\n",
    "pretrain = False\n",
    "train_type = 'finetune'\n",
    "use_lab = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "code_fuse, visit_fuse = 'simple', 'simple'\n",
    "loss = 'pos_weight'\n",
    "gnn_type = model_config['GNN']['type']\n",
    "code_range = 'all'\n",
    "\n",
    "pos_weight, neg_weight = 1, 1\n",
    "print(f'Loss configuration: pos_weight={pos_weight}, neg_weight={neg_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Graph Construction\n",
    "\n",
    "The `load_data` function retrieves preprocessed tensors and graph structures. The graph remains static during training, serving as a structural prior for disease relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease graph nodes: 7946\n",
      "Train samples: 8000\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_codes_x, train_codes_y, test_codes_x, test_codes_y,\n",
    "    train_labs_x, test_labs_x, edge_index, x, edge_weight,\n",
    "    train_codes_y_r, test_codes_y_r\n",
    ") = load_data(pretrain, data_path, dataset, model_config['init_dim'])\n",
    "\n",
    "x = x.float()\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "print('Disease graph nodes:', data.x.shape[0])\n",
    "print('Train samples:', train_codes_x.shape[0])\n",
    "print('Test samples:', test_codes_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PatientDataset(train_codes_x, train_labs_x.float(), train_codes_y)\n",
    "test_dataset = PatientDataset(test_codes_x, test_labs_x.float(), test_codes_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization with Pretrained Weights\n",
    "\n",
    "The model is initialized with:\n",
    "1. Pretrained graph encoder parameters (from laboratory pretraining)\n",
    "2. Category-specific decoder weights (hematology, chemistry, blood gas)\n",
    "3. Auxiliary laboratory layer weights (from train_lab phase)\n",
    "\n",
    "For MIMIC-IV, `num_classes` follows: [239 hematology, 191 chemistry, 15 blood gas, total diagnostic codes]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading auxiliary laboratory layer weights...\n",
      "DuaLK(\n",
      "  (gnn_layer): GATLayer(\n",
      "    (conv1): GATConv(2000, 256, heads=1)\n",
      "    (conv2): GATConv(256, 256, heads=1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (visit_attention): AttentionLayer(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (patient_attention): AttentionLayer(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lab_layer): Linear(in_features=290, out_features=256, bias=True)\n",
      "  (decoder1): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=239, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (decoder2): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=191, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (decoder3): Decoder(\n",
      "    (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=15, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (fc1): Linear(in_features=896, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=7337, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 50  # Here we set 50 epochs for quick start\n",
    "\n",
    "if dataset == 'mimic3':\n",
    "    num_classes = [159, 115, 16, train_codes_y.shape[1]]\n",
    "elif dataset == 'mimic4':\n",
    "    num_classes = [239, 191, 15, train_codes_y.shape[1]]\n",
    "else:\n",
    "    raise ValueError('Invalid dataset')\n",
    "\n",
    "if use_lab:\n",
    "    print('Loading auxiliary laboratory layer weights...')\n",
    "    model_path = './saved/train_lab/lab_layer_checkpoint_end.pth'\n",
    "    checkpoint = torch.load(model_path)\n",
    "    lab_weight, lab_bias = checkpoint['linear1_weight'], checkpoint['linear1_bias']\n",
    "else:\n",
    "    lab_weight, lab_bias = None, None\n",
    "\n",
    "data = data.to(str(device))\n",
    "model = DuaLK(\n",
    "    model_config=model_config,\n",
    "    emb_init=data.x,\n",
    "    num_classes=num_classes,\n",
    "    use_lab=use_lab,\n",
    "    code_fuse=code_fuse,\n",
    "    visit_fuse=visit_fuse,\n",
    "    train_type=train_type,\n",
    "    lab_weight=lab_weight,\n",
    "    lab_bias=lab_bias,\n",
    "    gnn_type=gnn_type\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameters...\n",
      "Pretrained weights successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "if train_type in ['pretrain', 'finetune']:\n",
    "    print('Loading pretrained parameters...')\n",
    "    model.load_partial_state_dict(\n",
    "        torch.load(f'./saved/joint_pretrained_model_{joint_epochs}.pth', map_location=device)\n",
    "    )\n",
    "    model.load_decoder_weights(\n",
    "        './saved/decoder_hema_weights.pth',\n",
    "        './saved/decoder_chem_weights.pth',\n",
    "        './saved/decoder_blood_weights.pth',\n",
    "        device\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print('Pretrained weights successfully loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = WeightedBCEWithLogitsLoss(pos_weight=pos_weight, neg_weight=neg_weight).to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "os.makedirs('./saved/train', exist_ok=True)\n",
    "torch.save(data.x.cpu(), './saved/train/initial_embeddings.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "The training employs a curriculum learning strategy via adaptive learning rate decay. Early epochs focus on coarse-grained pattern learning, while later epochs refine predictions with granular adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Commencing end-to-end training...\n",
      "\n",
      "Epoch 1/50, Train Loss: 0.02562, Test Loss: 0.00493, F1-weighted: 0.1370, Recall@10: 0.3303, Recall@20: 0.4005, Recall@30: 0.4536, Recall@40: 0.4874, LR: 0.001000\n",
      "Epoch 2/50, Train Loss: 0.00630, Test Loss: 0.00468, F1-weighted: 0.1576, Recall@10: 0.3480, Recall@20: 0.4209, Recall@30: 0.4794, Recall@40: 0.5226, LR: 0.001000\n",
      "Epoch 3/50, Train Loss: 0.00598, Test Loss: 0.00455, F1-weighted: 0.1756, Recall@10: 0.3652, Recall@20: 0.4382, Recall@30: 0.4962, Recall@40: 0.5388, LR: 0.001000\n",
      "Epoch 4/50, Train Loss: 0.00577, Test Loss: 0.00452, F1-weighted: 0.1727, Recall@10: 0.3638, Recall@20: 0.4337, Recall@30: 0.4998, Recall@40: 0.5491, LR: 0.001000\n",
      "Epoch 5/50, Train Loss: 0.00564, Test Loss: 0.00445, F1-weighted: 0.1844, Recall@10: 0.3737, Recall@20: 0.4454, Recall@30: 0.5133, Recall@40: 0.5587, LR: 0.001000\n",
      "Epoch 6/50, Train Loss: 0.00553, Test Loss: 0.00441, F1-weighted: 0.1960, Recall@10: 0.3832, Recall@20: 0.4551, Recall@30: 0.5164, Recall@40: 0.5625, LR: 0.001000\n",
      "Epoch 7/50, Train Loss: 0.00546, Test Loss: 0.00446, F1-weighted: 0.2064, Recall@10: 0.3848, Recall@20: 0.4621, Recall@30: 0.5163, Recall@40: 0.5562, LR: 0.001000\n",
      "Epoch 8/50, Train Loss: 0.00539, Test Loss: 0.00438, F1-weighted: 0.1993, Recall@10: 0.3844, Recall@20: 0.4600, Recall@30: 0.5189, Recall@40: 0.5644, LR: 0.001000\n",
      "Epoch 9/50, Train Loss: 0.00532, Test Loss: 0.00434, F1-weighted: 0.2114, Recall@10: 0.3960, Recall@20: 0.4738, Recall@30: 0.5280, Recall@40: 0.5763, LR: 0.001000\n",
      "Epoch 10/50, Train Loss: 0.00526, Test Loss: 0.00433, F1-weighted: 0.2136, Recall@10: 0.3932, Recall@20: 0.4729, Recall@30: 0.5270, Recall@40: 0.5735, LR: 0.001000\n",
      "Epoch 11/50, Train Loss: 0.00519, Test Loss: 0.00431, F1-weighted: 0.2178, Recall@10: 0.3995, Recall@20: 0.4749, Recall@30: 0.5357, Recall@40: 0.5807, LR: 0.001000\n",
      "Epoch 12/50, Train Loss: 0.00513, Test Loss: 0.00429, F1-weighted: 0.2215, Recall@10: 0.4054, Recall@20: 0.4775, Recall@30: 0.5400, Recall@40: 0.5811, LR: 0.001000\n",
      "Epoch 13/50, Train Loss: 0.00508, Test Loss: 0.00433, F1-weighted: 0.2244, Recall@10: 0.4026, Recall@20: 0.4813, Recall@30: 0.5417, Recall@40: 0.5867, LR: 0.001000\n",
      "Epoch 14/50, Train Loss: 0.00503, Test Loss: 0.00428, F1-weighted: 0.2294, Recall@10: 0.4082, Recall@20: 0.4843, Recall@30: 0.5426, Recall@40: 0.5872, LR: 0.001000\n",
      "Epoch 15/50, Train Loss: 0.00496, Test Loss: 0.00428, F1-weighted: 0.2321, Recall@10: 0.4118, Recall@20: 0.4845, Recall@30: 0.5440, Recall@40: 0.5859, LR: 0.001000\n",
      "Epoch 16/50, Train Loss: 0.00491, Test Loss: 0.00431, F1-weighted: 0.2319, Recall@10: 0.4092, Recall@20: 0.4859, Recall@30: 0.5443, Recall@40: 0.5903, LR: 0.001000\n",
      "Epoch 17/50, Train Loss: 0.00484, Test Loss: 0.00427, F1-weighted: 0.2386, Recall@10: 0.4186, Recall@20: 0.4864, Recall@30: 0.5480, Recall@40: 0.5955, LR: 0.001000\n",
      "Epoch 18/50, Train Loss: 0.00479, Test Loss: 0.00428, F1-weighted: 0.2393, Recall@10: 0.4129, Recall@20: 0.4905, Recall@30: 0.5499, Recall@40: 0.5978, LR: 0.001000\n",
      "Epoch 19/50, Train Loss: 0.00471, Test Loss: 0.00427, F1-weighted: 0.2436, Recall@10: 0.4255, Recall@20: 0.4954, Recall@30: 0.5576, Recall@40: 0.6008, LR: 0.001000\n",
      "Epoch 20/50, Train Loss: 0.00465, Test Loss: 0.00426, F1-weighted: 0.2455, Recall@10: 0.4221, Recall@20: 0.4948, Recall@30: 0.5591, Recall@40: 0.6004, LR: 0.001000\n",
      "Epoch 21/50, Train Loss: 0.00459, Test Loss: 0.00427, F1-weighted: 0.2518, Recall@10: 0.4267, Recall@20: 0.4996, Recall@30: 0.5608, Recall@40: 0.6003, LR: 0.001000\n",
      "Epoch 22/50, Train Loss: 0.00453, Test Loss: 0.00427, F1-weighted: 0.2482, Recall@10: 0.4260, Recall@20: 0.4983, Recall@30: 0.5580, Recall@40: 0.6001, LR: 0.001000\n",
      "Epoch 23/50, Train Loss: 0.00448, Test Loss: 0.00429, F1-weighted: 0.2515, Recall@10: 0.4267, Recall@20: 0.4991, Recall@30: 0.5587, Recall@40: 0.6024, LR: 0.001000\n",
      "Epoch 24/50, Train Loss: 0.00443, Test Loss: 0.00429, F1-weighted: 0.2502, Recall@10: 0.4249, Recall@20: 0.4929, Recall@30: 0.5574, Recall@40: 0.6023, LR: 0.001000\n",
      "Epoch 25/50, Train Loss: 0.00436, Test Loss: 0.00429, F1-weighted: 0.2599, Recall@10: 0.4325, Recall@20: 0.5032, Recall@30: 0.5599, Recall@40: 0.6039, LR: 0.001000\n",
      "Epoch 26/50, Train Loss: 0.00430, Test Loss: 0.00431, F1-weighted: 0.2617, Recall@10: 0.4321, Recall@20: 0.5015, Recall@30: 0.5625, Recall@40: 0.6077, LR: 0.001000\n",
      "Epoch 27/50, Train Loss: 0.00425, Test Loss: 0.00434, F1-weighted: 0.2582, Recall@10: 0.4318, Recall@20: 0.5004, Recall@30: 0.5601, Recall@40: 0.6031, LR: 0.001000\n",
      "Epoch 28/50, Train Loss: 0.00420, Test Loss: 0.00431, F1-weighted: 0.2588, Recall@10: 0.4372, Recall@20: 0.5041, Recall@30: 0.5637, Recall@40: 0.6085, LR: 0.001000\n",
      "Epoch 29/50, Train Loss: 0.00412, Test Loss: 0.00432, F1-weighted: 0.2637, Recall@10: 0.4364, Recall@20: 0.5063, Recall@30: 0.5668, Recall@40: 0.6075, LR: 0.001000\n",
      "Epoch 30/50, Train Loss: 0.00408, Test Loss: 0.00433, F1-weighted: 0.2655, Recall@10: 0.4381, Recall@20: 0.5065, Recall@30: 0.5661, Recall@40: 0.6160, LR: 0.001000\n",
      "Epoch 31/50, Train Loss: 0.00402, Test Loss: 0.00437, F1-weighted: 0.2663, Recall@10: 0.4312, Recall@20: 0.5062, Recall@30: 0.5655, Recall@40: 0.6095, LR: 0.001000\n",
      "Epoch 32/50, Train Loss: 0.00390, Test Loss: 0.00437, F1-weighted: 0.2697, Recall@10: 0.4413, Recall@20: 0.5120, Recall@30: 0.5747, Recall@40: 0.6213, LR: 0.000500\n",
      "Epoch 33/50, Train Loss: 0.00385, Test Loss: 0.00434, F1-weighted: 0.2704, Recall@10: 0.4425, Recall@20: 0.5128, Recall@30: 0.5692, Recall@40: 0.6154, LR: 0.000500\n",
      "Epoch 34/50, Train Loss: 0.00382, Test Loss: 0.00436, F1-weighted: 0.2694, Recall@10: 0.4414, Recall@20: 0.5135, Recall@30: 0.5762, Recall@40: 0.6175, LR: 0.000500\n",
      "Epoch 35/50, Train Loss: 0.00378, Test Loss: 0.00439, F1-weighted: 0.2734, Recall@10: 0.4435, Recall@20: 0.5167, Recall@30: 0.5757, Recall@40: 0.6169, LR: 0.000500\n",
      "Epoch 36/50, Train Loss: 0.00375, Test Loss: 0.00439, F1-weighted: 0.2731, Recall@10: 0.4416, Recall@20: 0.5172, Recall@30: 0.5765, Recall@40: 0.6176, LR: 0.000500\n",
      "Epoch 37/50, Train Loss: 0.00373, Test Loss: 0.00439, F1-weighted: 0.2761, Recall@10: 0.4428, Recall@20: 0.5152, Recall@30: 0.5761, Recall@40: 0.6185, LR: 0.000500\n",
      "Epoch 38/50, Train Loss: 0.00369, Test Loss: 0.00441, F1-weighted: 0.2762, Recall@10: 0.4414, Recall@20: 0.5154, Recall@30: 0.5767, Recall@40: 0.6190, LR: 0.000500\n",
      "Epoch 39/50, Train Loss: 0.00367, Test Loss: 0.00443, F1-weighted: 0.2765, Recall@10: 0.4416, Recall@20: 0.5116, Recall@30: 0.5718, Recall@40: 0.6161, LR: 0.000500\n",
      "Epoch 40/50, Train Loss: 0.00365, Test Loss: 0.00441, F1-weighted: 0.2766, Recall@10: 0.4442, Recall@20: 0.5162, Recall@30: 0.5783, Recall@40: 0.6191, LR: 0.000500\n",
      "Epoch 41/50, Train Loss: 0.00362, Test Loss: 0.00442, F1-weighted: 0.2782, Recall@10: 0.4475, Recall@20: 0.5177, Recall@30: 0.5753, Recall@40: 0.6186, LR: 0.000500\n",
      "Epoch 42/50, Train Loss: 0.00353, Test Loss: 0.00442, F1-weighted: 0.2809, Recall@10: 0.4480, Recall@20: 0.5177, Recall@30: 0.5764, Recall@40: 0.6200, LR: 0.000100\n",
      "Epoch 43/50, Train Loss: 0.00350, Test Loss: 0.00443, F1-weighted: 0.2810, Recall@10: 0.4505, Recall@20: 0.5186, Recall@30: 0.5798, Recall@40: 0.6218, LR: 0.000100\n",
      "Epoch 44/50, Train Loss: 0.00350, Test Loss: 0.00443, F1-weighted: 0.2819, Recall@10: 0.4493, Recall@20: 0.5197, Recall@30: 0.5797, Recall@40: 0.6229, LR: 0.000100\n",
      "Epoch 45/50, Train Loss: 0.00349, Test Loss: 0.00445, F1-weighted: 0.2815, Recall@10: 0.4506, Recall@20: 0.5203, Recall@30: 0.5820, Recall@40: 0.6252, LR: 0.000100\n",
      "Epoch 46/50, Train Loss: 0.00348, Test Loss: 0.00445, F1-weighted: 0.2830, Recall@10: 0.4511, Recall@20: 0.5207, Recall@30: 0.5818, Recall@40: 0.6229, LR: 0.000100\n",
      "Epoch 47/50, Train Loss: 0.00347, Test Loss: 0.00443, F1-weighted: 0.2826, Recall@10: 0.4476, Recall@20: 0.5207, Recall@30: 0.5804, Recall@40: 0.6234, LR: 0.000100\n",
      "Epoch 48/50, Train Loss: 0.00347, Test Loss: 0.00444, F1-weighted: 0.2834, Recall@10: 0.4484, Recall@20: 0.5178, Recall@30: 0.5807, Recall@40: 0.6254, LR: 0.000100\n",
      "Epoch 49/50, Train Loss: 0.00345, Test Loss: 0.00446, F1-weighted: 0.2823, Recall@10: 0.4503, Recall@20: 0.5178, Recall@30: 0.5828, Recall@40: 0.6225, LR: 0.000100\n",
      "Epoch 50/50, Train Loss: 0.00345, Test Loss: 0.00445, F1-weighted: 0.2821, Recall@10: 0.4497, Recall@20: 0.5186, Recall@30: 0.5814, Recall@40: 0.6242, LR: 0.000100\n",
      "\n",
      "Training completed. Final checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "print('\\nCommencing end-to-end training...\\n')\n",
    "for epoch in range(epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, learning_rate)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        patient_data, labels = batch\n",
    "        patient_data = {k: v.to(device) for k, v in patient_data.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, patient_data)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            patient_data, labels = batch\n",
    "            patient_data = {k: v.to(device) for k, v in patient_data.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(data, patient_data)\n",
    "            loss = criterion(output, labels.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            y_true.append(labels.cpu().numpy())\n",
    "            y_pred.append(output.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    y_pred_sorted = np.argsort(y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "    f1_weighted = f1(y_true, y_pred_sorted, metrics='weighted')\n",
    "    ks = [10, 20, 30, 40] if code_range == 'all' else [5, 8, 15]\n",
    "    _, recall_at_k = top_k_prec_recall(y_true, y_pred_sorted, ks)\n",
    "\n",
    "    recall_str = \", \".join([f\"Recall@{k}: {recall:.4f}\" for k, recall in zip(ks, recall_at_k)])\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.5f}, Test Loss: {test_loss:.5f}, '\n",
    "          f'F1-weighted: {f1_weighted:.4f}, {recall_str}, LR: {current_lr:.6f}')\n",
    "\n",
    "    torch.save(model.state_dict(), './saved/train/checkpoint.pth')\n",
    "\n",
    "print('\\nTraining completed. Final checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the complete DuaLK pipeline on MIMIC-IV:\n",
    "1. **Preprocessing**: Raw EHR â structured tensors + disease graph\n",
    "2. **Laboratory Pretraining**: Multi-category lab prediction with graph-aware encoders\n",
    "3. **Auxiliary Lab Training**: Direct lab-diagnosis mapping\n",
    "4. **Fine-tuning**: Integrated model with dual knowledge (graph + lab)\n",
    "\n",
    "The modular design enables flexible experimentation with different graph encoders, fusion strategies, and pretraining objectives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
